["```\nREPOSITORY TAG IMAGE ID CREATED SIZE\n```", "```\nCannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\n```", "```\n    ---\n    - hosts: localhost\n      tasks:\n      - name: Start a container with a command\n        community.docker.docker_container:\n          name: test-container\n          image: alpine\n          command:\n          - echo\n          - \"Hello, World!\"\n    ```", "```\n    $ ansible-playbook start-docker-container.yaml\n    ```", "```\nPLAY [localhost] *********************************************************************\nTASK [Gathering Facts] ***************************************************************\nok: [localhost]\nTASK [Start a container with a command] **********************************************\nchanged: [localhost]\nPLAY RECAP ***************************************************************************\nlocalhost : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0\n```", "```\n    $ docker container list -a\n    ```", "```\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES\nc706ec55fc0d alpine \"echo Hello, World!\" 3 minutes ago Exited (0) About a minute ago test-container\n```", "```\n    $ docker logs test-container\n    ```", "```\ncommunity.docker.docker_container module. This is not the only module Ansible has to control the Docker daemon, but it is probably one of the most widely used since it’s used to control containers running on Docker.\nOther modules include the following:\n\n*   `community.docker.docker_config`: Used to change the configurations of the Docker daemon\n*   `community.docker.docker_container_info`: Used to gather information from (inspect) a container\n*   `community.docker.docker_network`: Used to manage Docker networking configuration\n\nThere are also many modules in the `community.docker` collection, but they are actually used to manage Docker Swarm clusters and not Docker instances. Some examples are as follows:\n\n*   `community.docker.docker_node`: Used to manage a node in a Docker Swarm cluster\n*   `community.docker.docker_node_info`: Used to retrieve information about a specific node in a Docker Swarm cluster\n*   `community.docker.docker_swarm_info`: Used to retrieve information about a Docker Swarm cluster\n\nAs we will see in the next section, there are many more modules that can be used to manage containers that are orchestrated in various ways.\nNow that you have learned how to automate Docker with Ansible, we will see how to work with Podman, a Docker alternative.\nManaging Podman\nPodman is a container engine competitor to Docker. The reason why Red Hat started to develop an alternative container engine is due to the design of Docker. Although Docker changed some design decisions over the years, at the time, Docker was running as a daemon that required root access to the machine, and the CLI `docker` command was just a client. The risk with this approach, at least from Red Hat’s standpoint, was that a bug in the Docker daemon could have endangered the whole system’s security.\nPodman, differently from Docker, does not have a daemon running, and all utilities Podman provides are discrete. This approach allows Podman to be much more secure than Docker, allowing containers to be run without any root access.\nOne of the critical decisions made at the very beginning of Podman development was the CLI and API compatibility between Podman and Docker. This aspect dramatically increases the simplicity of moving from one to the other without much effort.\nAs you might have imagined, from an Ansible standpoint, it is also going to be very similar.\nDue to the simpler design Podman has, to ensure that it is present and usable on your system, it is enough to run the `podman` command. If it returns the following, it means that you do not have it installed:\n\n```", "```\n\n If it returns all the command options, this means that Podman is properly installed on your system.\nWe can therefore create our first Ansible Playbook to manage Podman, which we will call `start-podman-container.yaml`, with the following content:\n\n```", "```\n\n We can now run the playbook by executing the following:\n\n```", "```\n\n As you may expect, it will give you an output similar to this:\n\n```", "```\n\n As we did for Docker, we can check that the command executed properly by running the following:\n\n```", "```\n\n And this command will return the information of the container that ran:\n\n```", "```\n\n The fact that it exited with `0` means that it successfully ran, as we were expecting.\nSimilarly to Docker, there are many other modules usable in the `containers.podman` collection. Differently from Docker, though, there is no such thing as Podman Swarm, and therefore all Docker modules focusing on Swarm will not have a Podman counterpart.\nNow that we have explored the two major standalone container engines that you might encounter in development and test environments, we will discuss Kubernetes since it is the most widely used container platform in production environments.\nManaging Kubernetes with Ansible\nWe will assume that you have access to either a Kubernetes or OpenShift cluster for testing. Setting these up is out of the scope of this book, so you might want to look at a distribution such as Minikube or Minishift, both of which are designed to be quick and easy to set up so that you can start learning these technologies rapidly. We also need to have the `kubectl` client or the `oc` client, depending on whether we have deployed Kubernetes or OpenShift, properly configured.\nInstalling Ansible Kubernetes dependencies\nFirst of all, you need to ensure you are running a fairly updated version of Python 3 (>=3.6) and install the required Python packages (you can install it either via PIP or your OS packaging system):\n\n*   Kubernetes >= 12.0.0\n*   PyYAML >= 3.11\n*   `jsonpatch`\n\nSince the `kubernetes.core` collection is not generally shipped with Ansible, you’ll need to install it by running this:\n\n```", "```\n\n We are now ready for our first Kubernetes playbook!\nListing Kubernetes namespaces with Ansible\nA Kubernetes cluster has multiple namespaces internally, and you can usually find the ones a cluster has with `kubectl get namespaces`. You can do the same with Ansible by creating a file called `k8s-ns-show.yaml` with the following content:\n\n```", "```\n\n We can now execute this as follows:\n\n```", "```\n\n You will now see information regarding the namespaces in the output.\nNotice that in the seventh line of the code (`kind: Namespace`), we are specifying the type of resources we are interested in. You can specify other Kubernetes object types to see them (for example, you can try this with Deployments, Services, and Pods).\nCreating a Kubernetes namespace with Ansible\nSo far, we have learned how to show existing namespaces, but usually, Ansible is used in a declarative way to achieve a desired state. So, let’s create a new playbook called `k8s-ns.yaml` with the following content:\n\n```", "```\n\n Before running it, we can execute `kubectl get ns` so that we can ensure `myns` is not present. In my case, the output is as follows:\n\n```", "```\n\n We can now run the playbook with the following command:\n\n```", "```\n\n The output should resemble the following one:\n\n```", "```\n\n As you can see, Ansible reports that it changed the namespace state. If I execute `kubectl get ns` again, it is clear that Ansible created the namespace we were expecting:\n\n```", "```\n\n Now, let’s create a service.\nCreating a Kubernetes service with Ansible\nSo far, we have seen how to create namespaces from Ansible, so now, let’s put a service in the namespace we just created. Let’s create a new playbook called `k8s-svc.yaml` with the following content:\n\n```", "```\n\n Before running it, we can execute `kubectl get svc` to ensure that the namespace has no services. Make sure you’re in the right namespace before running it! In my case, the output is as follows:\n\n```", "```\n\n We can now run it with the following command:\n\n```", "```\n\n The output should resemble the following one:\n\n```", "```\n\n As you can see, Ansible reports that it changed the service state. If I execute `kubectl get svc` again, it is clear that Ansible created the service we were expecting:\n\n```", "```\n\n As you can see, we followed the same procedure that we used in the namespace case, but we specified a different Kubernetes object type and specified the various parameters that are needed for the Service type. You can do the same for all other Kubernetes object types.\nNow that you have learned how to deal with Kubernetes clusters, you’ll learn how to automate Docker with Ansible.\nExploring container-focused modules\nOften, when organizations grow, they start to use multiple technologies in different parts of the organization. Another thing that usually happens is that after a department has found that a vendor worked well for them, they will be more inclined to try new technologies offered by that vendor. A mix of those two factors and time (usually, fewer technology cycles) will end up creating multiple solutions for the same problem within the same organization.\nIf your organization is in this situation with containers, Ansible can come to the rescue, thanks to its ability to interoperate with the majority of, if not all, container platforms.\nOften, the biggest obstacle to doing something with Ansible is finding the name of the modules you need to use to achieve what you want to achieve. In this section, we will try to help in this effort, mainly in terms of the containerization space, but this might help you in the quest to find different kinds of modules.\nThe starting point for all Ansible module research should be the official Ansible documentation and eventually Ansible Galaxy.\nMany Ansible modules relative to container services are in the cloud collections category (ECS, Docker, LXC, or LXD), containers (at the moment only Podman), or in their own one, such as Kubernetes.\nTo help you further, let’s take a look at some of the main container platforms and the main modules Ansible provides.\n**Amazon Web Services** (**AWS**), back in 2014, launched **Elastic Container Service** (**ECS**), which is a way to deploy and orchestrate Docker containers within their infrastructure. In the following year, AWS also launched **Elastic Container Registry** (**ECR**), a managed Docker Registry service. The service did not become as ubiquitous as AWS hoped, so in 2018, AWS launched **Elastic Kubernetes Service** (**EKS**) to allow people that wanted to run Kubernetes on AWS to have a managed service.\nIf you are using or plan to use EKS, this is just a standard managed Kubernetes cluster, so you can use the Kubernetes-specific modules that we are going to cover shortly. If you decide to use ECS, there are several modules that could help you. The most important ones are `community.aws.ecs_cluster`, which allows you to create or terminate ECS clusters; `community.aws.ecs_ecr`, which allows you to manage ECR; `community.aws.ecs_service`, which allows you to create, terminate, start, or stop a service in ECS; and `community.aws.ecs_task`, which allows you to run, start, or stop a task in ECS. In addition to those, there is `community.aws.ecs_service_info`, which allows Ansible to list or describe services in ECS.\nMicrosoft Azure, in 2018, announced `azure.azcollection.azure_rm_aks` module allows us to create, update, and delete AKS instances.\nGoogle Cloud launched **Google Kubernetes Engine** (**GKE**) in 2015\\. GKE is the **Google Cloud Platform**(**GCP**) version of managed Kubernetes, and is therefore compatible with Ansible Kubernetes modules. In addition to those, there are various GKE-specific modules, some of which are as follows:\n\n*   `google.cloud.gcp_container_cluster`: Allows you to create a GCP cluster\n*   `google.cloud.gcp_container_cluster_info`: Allows you to gather facts for a GCP cluster\n*   `google.cloud.gcp_container_node_pool`: Allows you to create a GCP node pool\n*   `google.cloud.gcp_container_node_pool_info`: Allows you to gather facts for a GCP node pool\n\nRed Hat started OpenShift in 2011, and at the time, it was based on its own container runtime. Version 3, which was released in 2015, was completely based on Kubernetes, so all Ansible Kubernetes modules work. In addition to those, there is the `oc` module, which is currently still present but in a deprecated state, giving preference to the Kubernetes modules.\nIn 2015, Google released Kubernetes and, quickly, a huge community started to build around it. Ansible allows you to manage your Kubernetes clusters with some modules:\n\n*   `kubernetes.core.k8s`: Allows you to manage any kind of Kubernetes object\n*   `kubernetes.core.k8s_auth`: Allows you to authenticate to Kubernetes clusters that require an explicit login step\n*   `kubernetes.core.k8s_facts`: Allows you to inspect Kubernetes objects\n*   `kubernetes.core.k8s_scale`: Allows you to set a new size for a Deployment, ReplicaSet, Replication Controller, or Job\n*   `kubernetes.core.k8s_service`: Allows you to manage Services on Kubernetes\n\nLXC and LXD are also systems that can be used to run containers in Linux. These systems are also supported by Ansible, thanks to the following modules:\n\n*   `community.general.lxc_container`: Allows you to manage LXC containers\n*   `community.general.lxd_container`: Allows you to manage LXD containers\n*   `community.general.lxd_profile`: Allows you to manage LXD profiles\n\nNow that you have learned how to explore container-focused modules, you’ll learn how to automate with AWS.\nAutomating with Amazon Web Services\nIn many organizations, cloud providers are used widely, while in others, they are just being introduced. However, in one way or another, you will probably have to deal with a cloud provider while doing your job. AWS is the biggest and oldest, and is perhaps something you will have to work with.\nInstallation\nTo be able to use Ansible to automate your AWS estate, you’ll need to install the `boto3` library. To do so, run the following command:\n\n```", "```\n\n As for collections, at the moment, there are two collections to interact with AWS services: `community.aws` and `amazon.aws`. In many cases, you will need both of them, since many features of the former have not yet been implemented in the latter:\n\n```", "```\n\n Now that you have all the necessary software installed, you can set up authentication.\nAuthentication\nThe `boto` library looks up the necessary credentials in the `~/.aws/credentials` file. There are two different ways to ensure that the credentials file is configured properly.\nIt is possible to use the AWS CLI tool. Alternatively, this can be done with a text editor of your choice by creating a file with the following structure:\n\n```", "```\n\n Now that you’ve created the file with the necessary credentials, `boto` will be able to work against your AWS environment. Since Ansible uses `boto` for every single communication with AWS systems, this means that Ansible will be appropriately configured, even without you having to change any Ansible-specific configuration.\nCreating your first machine\nNow that Ansible is able to connect to your AWS environment, you can proceed with the actual playbook by following these steps:\n\n1.  Create the `aws.yaml` Playbook with the following content, ensuring you use your public key in the `key_material` value in the first task:\n\n    ```", "```\n\n     2.  Run it using the following command:\n\n    ```", "```\n\nThis command will return something like the following:\n\n```", "```\n\n If you check the AWS console, you will see that you now have one machine up and running!\nTo launch a virtual machine in AWS, we need a few things to be in place:\n\n*   An SSH key pair\n*   A network\n*   A subnetwork\n*   A security group\n\nBy default, a network and a subnetwork are already available in your account, but you need to retrieve their IDs.\nThat’s why we started by uploading the public part of an SSH key pair to AWS, queried for information about the network and the subnetwork, then ensured that the security group we wanted to use was present, and lastly triggered the machine build.\nNow you have learned how to automate against AWS, you’ll learn how to complement GCP with automation.\nComplementing Google Cloud Platform with automation\nAnother global cloud provider is Google, with its GCP. Google’s approach to the cloud is different from other providers’ since Google does not try to simulate the data center in a virtual environment. This is because Google wants to rethink the concept of cloud provision in order to simplify it.\nInstallation\nYou need to ensure that you have the proper components installed before you can start using GCP with Ansible. More specifically, you will require the Python `requests` and `google-auth` modules. To install these modules, run the following command:\n\n```", "```\n\n We can now proceed to install the Google Cloud collection:\n\n```", "```\n\n Now that you have all the dependencies present, you can start the authentication process.\nAuthentication\nThere are three different approaches to obtaining a working set of credentials in GCP:\n\n*   The service account using environment variables\n*   The service account using a JSON file\n*   The machine account\n\nThe first two approaches are the suggested ones in the majority of cases since the third applies only to circumstances where Ansible is run directly within the GCP environment.\nThe first approach is, once you have created the service account, set the following environmental variables:\n\n*   `GCP_AUTH_KIND`\n*   `GCP_SERVICE_ACCOUNT_EMAIL`\n*   `GCP_SERVICE_ACCOUNT_FILE`\n*   `GCP_SCOPES`\n\nNow, Ansible can use the proper service account.\nIf you prefer not to use environment variables, you can download a service account file from the GCP interface directly. For convenience’s sake, we are going to do this and we are going to save the file as `~/sa.json`.\nThe third approach is by far the easiest since Ansible will be able to auto-detect the machine account if you are running it in a GCP instance.\nCreating your first machine\nNow that Ansible is able to connect to your GCP environment, you can proceed with the actual Playbook:\n\n1.  Create the `gce.yaml` Playbook with the following content:\n\n    ```", "```\n\n     2.  Execute it with the following command:\n\n    ```", "```\n\nThis will create an output like the following one:\n\n```", "```\n\n As for the AWS example, running a machine in the cloud is very easy with Ansible.\nIn the case of GCE, you don’t need to set up the networks beforehand since the GCE defaults will kick in and provide a functional machine either way.\nAs for AWS, the list of modules you can use is huge. You can find the full list at [https://docs.ansible.com/ansible/latest/collections/google/cloud/index.xhtml](https://docs.ansible.com/ansible/latest/collections/google/cloud/index.xhtml).\nNow that you have learned how to complement GCP with automation, you will learn how to seamlessly perform automation integration with Azure.\nSeamless automation integration with Azure\nAnother global cloud that Ansible can manage is Microsoft Azure.\nAzure integration, like AWS integration, requires quite a few steps to be performed in Playbooks.\nThe first thing you will need to do is set up the authentication so that Ansible is allowed to control your Azure account.\nInstallation\nTo let Ansible manage the Azure cloud, you need to install the Azure SDK for Python. Do this by executing the following command:\n\n```", "```\n\n We can now proceed to install the Azure collection:\n\n```", "```\n\n Now that you have all the dependencies present, you can start the authentication process.\nAuthentication\nThere are different ways to ensure that Ansible is able to manage Azure for you, based on the way your Azure account is set up, but they can all be configured in the `~/.``azure/credentials` file.\nIf you want Ansible to use the principal credentials for the Azure account, you will need to create a file that resembles the following:\n\n```", "```\n\n If you prefer to use Active Directory with a username and password, you can do something like this:\n\n```", "```\n\n Lastly, you can opt for an Active Directory login with ADFS. In this case, you’ll need to set some additional parameters. You’ll end up with something like this:\n\n```", "```\n\n The same parameters can be passed as parameters or as environmental variables if it makes more sense.\nCreating your first machine\nNow that Ansible is able to connect to your Azure environment, you can proceed with the actual Playbook:\n\n1.  Create the `azure.yaml` Playbook with the following content:\n\n    ```", "```\n\n     2.  We can run it with the following command:\n\n    ```", "```\n\nThis will return something like the following:\n\n```", "```\n\n You now have your machine running in the Azure cloud!\nAs you can see, in Azure, you will need all the resources to be ready before you can issue the machine creation command. This is the reason you create the storage account, the virtual network, the subnet, the public IP, the security group, and the NIC first, and only at that point, the machine itself.\nOutside the three major players in the market, there are many additional cloud options. Many of which, including many private clouds, leverage OpenStack.\nUsing Ansible to orchestrate OpenStack\nAs opposed to the various cloud services we just discussed, all of which are public clouds, OpenStack allows you to create your own (private) cloud.\nPrivate clouds have the disadvantage that they expose more complexity to the administrator and the user, but this is the reason they can be customized to suit an organization perfectly.\nInstallation\nThe first step to being able to control an OpenStack cluster with Ansible is to ensure that `openstacksdk` is installed.\nTo install `openstacksdk`, you need to execute the following command:\n\n```", "```\n\n We can now proceed to install the OpenStack collection:\n\n```", "```\n\n Now that you have installed `openstacksdk`, you can start the authentication process.\nAuthentication\nSince Ansible will use `openstacksdk` as its backend, you will need to ensure that `openstacksdk` is able to connect to the OpenStack cluster.\nTo do this, you can change the `~/.config/openstack/clouds.yaml` file, ensuring that there is a configuration for the cloud you want to use it for.\nAn example of what a correct OpenStack credentials set could look like is as follows:\n\n```", "```\n\n It’s also possible to set a different config file location if you are willing to export the `OS_CLIENT_CONFIG_FILE` variable as an environment variable.\nNow that you have set up the required security so that Ansible can manage your cluster, you can create your first Playbook.\nCreating your first machine\nSince OpenStack is very flexible, many of its components can have many different implementations, which means they may differ slightly in terms of their behavior. To be able to accommodate all the various cases, the Ansible modules that manage OpenStack tend to have a lower level of abstraction compared to the ones for many public clouds.\nSo, to create a machine, you will need to ensure that the public SSH key is known to OpenStack and ensure that the OS image is present as well. After doing this, you can set up networks, subnetworks, and routers to ensure that the machine we are going to create can communicate via the network. Then, you can create the security group and its rules so that the machine can receive connections (pings and SSH traffic, in this case). Finally, you can create a machine instance.\nTo complete all the steps just described, you need to create a file called `openstack.yaml` with the following content:\n\n```", "```\n\n Now, you can run it as follows:\n\n```", "```\n\n The output should be as follows:\n\n```", "```\n\n As you can see, this process was longer than the public cloud ones we covered. However, you did get to upload the image that you wanted to run, which is something many clouds do not allow (or allow with very complex processes).\nSummary\nIn this chapter, you learned how to automate tasks, such as managing Docker and Podman containers and managing Kubernetes objects. You also explored the modules that can help you automate cloud environments, such as AWS, GCP, Azure, and OpenStack. You also learned about the different approaches various cloud providers use, including their defaults and the parameters that you will always need to add.\nNow that you have an understanding of how Ansible interacts with the cloud, you can immediately start to automate your cloud workflows. Also, remember to check the documentation in the *Further reading* section to take a look at all the cloud modules that Ansible supports and their options.\nIn the next chapter, you will learn how to troubleshoot and create testing strategies.\nQuestions\n\n1.  Which of the following is NOT a GKE Ansible module?\n    1.  `google.cloud.gcp_container_cluster`\n    2.  `google.cloud.gcp_container_node_pool`\n    3.  `google.cloud.gcp_container_node_pool_info`\n    4.  `google.cloud.gcp_container_node_pool_count`\n    5.  `google.cloud.gcp_container_cluster_info`\n2.  True or false: In order to manage containers in Kubernetes, you need to add `k8s_namespace` in the settings section.\n    1.  True\n    2.  False\n3.  True or false: When working with Azure, instances require a **Network Interface Controller** (**NIC**) to be able to connect to the network.\n    1.  True\n    2.  False\n4.  True or false: When working with AWS, it’s necessary to create a Security Group before creating an EC2 instance.\n    1.  True\n    2.  False\n\nFurther reading\n\n*   More Docker containers: [https://docs.ansible.com/ansible/latest/collections/community/docker/index.xhtml](https://docs.ansible.com/ansible/latest/collections/community/docker/index.xhtml%0D)\n*   More Podman modules: [https://docs.ansible.com/ansible/latest/collections/containers/podman/index.xhtml](https://docs.ansible.com/ansible/latest/collections/containers/podman/index.xhtml%0D)\n*   More Kubernetes modules: [https://docs.ansible.com/ansible/latest/collections/kubernetes/core/index.xhtml](https://docs.ansible.com/ansible/latest/collections/kubernetes/core/index.xhtml%0D)\n*   More AWS modules:\n    *   [https://docs.ansible.com/ansible/latest/collections/amazon/aws/index.xhtml](https://docs.ansible.com/ansible/latest/collections/amazon/aws/index.xhtml)\n    *   [https://docs.ansible.com/ansible/latest/collections/community/aws/index.xhtml](https://docs.ansible.com/ansible/latest/collections/community/aws/index.xhtml)\n*   More GCP modules: [https://docs.ansible.com/ansible/latest/collections/google/cloud/index.xhtml](https://docs.ansible.com/ansible/latest/collections/google/cloud/index.xhtml%0D)\n*   More Azure modules: [https://docs.ansible.com/ansible/latest/collections/azure/azcollection/index.xhtml](https://docs.ansible.com/ansible/latest/collections/azure/azcollection/index.xhtml%0D)\n*   More OpenStack modules: [https://docs.ansible.com/ansible/latest/collections/openstack/cloud/index.xhtml](https://docs.ansible.com/ansible/latest/collections/openstack/cloud/index.xhtml)\n\n```"]